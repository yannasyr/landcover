{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from collections import OrderedDict\n",
    "from tifffile import TiffFile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import SegformerModel, SegformerConfig\n",
    "from transformers import  SegformerForSemanticSegmentation\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import copy \n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réécrit des fonctions utiles car les imports utilisent des parsers donc ça ne marche pas dans un Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverData():\n",
    "    \"\"\"Class to represent the S2GLC Land Cover Dataset for the challenge,\n",
    "    with useful metadata and statistics.\n",
    "    \"\"\"\n",
    "    # image size of the images and label masks\n",
    "    IMG_SIZE = 256\n",
    "    # the images are RGB+NIR (4 channels)\n",
    "    N_CHANNELS = 4\n",
    "    # we have 9 classes + a 'no_data' class for pixels with no labels (absent in the dataset)\n",
    "    N_CLASSES = 10\n",
    "    CLASSES = [\n",
    "        'no_data',\n",
    "        'clouds',\n",
    "        'artificial',\n",
    "        'cultivated',\n",
    "        'broadleaf',\n",
    "        'coniferous',\n",
    "        'herbaceous',\n",
    "        'natural',\n",
    "        'snow',\n",
    "        'water'\n",
    "    ]\n",
    "    # classes to ignore because they are not relevant. \"no_data\" refers to pixels without\n",
    "    # a proper class, but it is absent in the dataset; \"clouds\" class is not relevant, it\n",
    "    # is not a proper land cover type and images and masks do not exactly match in time.\n",
    "    IGNORED_CLASSES_IDX = [0, 1]\n",
    "\n",
    "    # The training dataset contains 18491 images and masks\n",
    "    # The test dataset contains 5043 images and masks\n",
    "    TRAINSET_SIZE = 18491\n",
    "    TESTSET_SIZE = 5043\n",
    "\n",
    "    # for visualization of the masks: classes indices and RGB colors\n",
    "    CLASSES_COLORPALETTE = {\n",
    "        0: [0,0,0],\n",
    "        1: [255,25,236],\n",
    "        2: [215,25,28],\n",
    "        3: [211,154,92],\n",
    "        4: [33,115,55],\n",
    "        5: [21,75,35],\n",
    "        6: [118,209,93],\n",
    "        7: [130,130,130],\n",
    "        8: [255,255,255],\n",
    "        9: [43,61,255]\n",
    "        }\n",
    "    CLASSES_COLORPALETTE = {c: np.asarray(color) for (c, color) in CLASSES_COLORPALETTE.items()}\n",
    "\n",
    "    # statistics\n",
    "    # the pixel class counts in the training set\n",
    "    TRAIN_CLASS_COUNTS = np.array(\n",
    "        [0, 20643, 60971025, 404760981, 277012377, 96473046, 333407133, 9775295, 1071, 29404605]\n",
    "    )\n",
    "    # the minimum and maximum value of image pixels in the training set\n",
    "    TRAIN_PIXELS_MIN = 1\n",
    "    TRAIN_PIXELS_MAX = 24356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y(mask2d):\n",
    "  occurrences = np.bincount(mask2d.flatten(), minlength=10)\n",
    "  Y = occurrences / np.sum(occurrences)\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_parse_image_mask(image_path):\n",
    "    \"\"\"Load an image and its segmentation mask as numpy arrays and returning a tuple\n",
    "    Args:\n",
    "        image_path : path to image\n",
    "    Returns:\n",
    "        (numpy.array[uint16], numpy.array[uint8]): the image and mask arrays\n",
    "    \"\"\"\n",
    "    # image_path = Path(image_path)\n",
    "    # get mask path from image path:\n",
    "    # image should be in a images/<image_id>.tif subfolder, while the mask is at masks/<image_id>.tif\n",
    "    mask_path = image_path.replace(\"images\",\"masks\")\n",
    "    with TiffFile(image_path) as tifi, TiffFile(mask_path) as tifm:\n",
    "        image = tifi.asarray()[:, :, :4] \n",
    "        mask = tifm.asarray()\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modifie la classe LandscapeData pour pouvoir insérer de la data augmentation sous conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapeData(Dataset):\n",
    "\n",
    "    def __init__(self, data_folder, transform=ToTensor(), transform_augm=None):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self.transform_augm = transform_augm\n",
    "\n",
    "        # Liste des noms de fichiers dans les dossiers\n",
    "        image_files = os.listdir(os.path.join(data_folder, 'images'))\n",
    "\n",
    "        # Utilisez numpy_parse_image_mask pour charger les images et les masques\n",
    "        self.train_data = [numpy_parse_image_mask(os.path.join(data_folder, 'images', filename)) for filename in image_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.train_data[idx]\n",
    "\n",
    "        image = image.astype(\"float32\")\n",
    "        mask = mask.astype(\"int64\")\n",
    "\n",
    "        # Memoire seuils : 0.3 / 0.3 / 0.3 / 0.6\n",
    "        seuil_per_water = 0.3\n",
    "        seuil_per_city = 0.3\n",
    "        seuil_per_natural = 0.3\n",
    "        seuil_per_conif = 0.6\n",
    "\n",
    "        Y = get_Y(mask) # Y[2] = 'artificial' ; Y[5] = 'coniferous' ; Y[7] = 'natural' ; Y[9] = 'water'\n",
    "\n",
    "        if Y[2] > seuil_per_city or Y[9] > seuil_per_water or Y[5] > seuil_per_conif  or Y[7] > seuil_per_natural :\n",
    "            # Augmentation de données\n",
    "            augmented = self.transform_augm(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "            print(\"AUGM:Type(image) = \", type(image), \" ; image.shape = \", image.shape)\n",
    "            print(\"AUGM:Type(mask) = \", type(mask), \" ; image.shape = \", mask.shape)\n",
    "\n",
    "        else:\n",
    "            # Pas d'augmentation, transformation simple sur l'image.\n",
    "            image = self.transform(image=image)['image']\n",
    "            mask = mask.astype(\"int64\")\n",
    "            mask = torch.tensor(mask, dtype=torch.int64) \n",
    "            mask = mask.squeeze()\n",
    "            print(\"Type(image) = \", type(image), \" ; image.shape = \", image.shape)\n",
    "            print(\"Type(mask) = \", type(mask), \" ; image.shape = \", mask.shape)\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici nos transformées : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "means =  [ 418.19976217,  703.34810956,  663.22678147, 3253.46844222]\n",
    "stds =  [294.73191962, 351.31328415, 484.47475774, 793.73928079]\n",
    "\n",
    "# Transformations \n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'train_augmentation': A.Compose([\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "        A.RandomRotate90(p=0.7),\n",
    "        A.Transpose(p=0.7),\n",
    "        # A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On pourrait rajouter dans l'augmentation cette transformation élastique, un peu violente ... ? \n",
    "# A.OneOf([\n",
    "# A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "# A.GridDistortion(p=0.5),\n",
    "# A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "# ], p=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé les datasets ```train```, ```validation```, ```test```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Number of images in the training set: 80\n",
      "Number of images in the validation set: 10\n",
      "Number of images in the test set: 10\n"
     ]
    }
   ],
   "source": [
    "# ------------- DATASET & DATALOADER ----------- \n",
    "\n",
    "# Définir le chemin du dossier d'entraînement\n",
    "data_folder = 'D:/my_git/landscape_data/dataset/small_dataset/'\n",
    "\n",
    "full_dataset = LandscapeData(data_folder, transform=None)  \n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.10 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "train_dataset.dataset.transform_augm = data_transforms['train_augmentation']\n",
    "val_dataset.dataset.transform = data_transforms['test']\n",
    "test_dataset.dataset.transform = data_transforms['test']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Number of images in train and val sets\n",
    "num_train_images = len(train_dataset)\n",
    "num_val_images = len(val_dataset)\n",
    "num_test_images = len(test_dataset)\n",
    "\n",
    "print(f\"Number of images in the training set: {num_train_images}\")\n",
    "print(f\"Number of images in the validation set: {num_val_images}\")\n",
    "print(f\"Number of images in the test set: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance un entrainement avec le *segformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segformer(lr=0.0001):\n",
    "    \n",
    "    config = SegformerConfig(\n",
    "        num_labels=10,\n",
    "        num_channels=4,\n",
    "        semantic_loss_ignore_index=0,\n",
    "        patch_sizes = [3, 2, 2, 2],\n",
    "        depths=[3, 4, 18, 3],\n",
    "        hidden_sizes=[64, 128, 320, 512],\n",
    "        decoder_hidden_size=768,\n",
    "    )\n",
    "    model_name =\"SegformerMit-B3\"\n",
    "\n",
    "    # Charger les poids pré-entrainés si le chemin est spécifié\n",
    "    model = SegformerForSemanticSegmentation(config)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "    return model,optimizer,model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,model_name, optimizer,scheduler, num_epochs,data_loaders, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    consecutive_epochs_no_improvement = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    dataset_sizes = {phase: len(data_loaders[phase].dataset) for phase in ['train', 'val']}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        running_loss = {'train': 0.0, 'val': 0.0}\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for inputs, targets in data_loaders[phase]:\n",
    "                pixel_values = inputs.to('cpu')\n",
    "                labels = targets.to('cpu')\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                    loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss[phase] += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss[phase] / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)  # Step the scheduler on validation loss\n",
    "\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "\n",
    "                # Check for early stopping\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    consecutive_epochs_no_improvement = 0\n",
    "                else:\n",
    "                    consecutive_epochs_no_improvement += 1\n",
    "\n",
    "        if consecutive_epochs_no_improvement >= patience:\n",
    "            print(f'Early stopping after {patience} consecutive epochs without improvement.')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return train_losses, val_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n",
      "Type(image) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([4, 256, 256])\n",
      "Type(mask) =  <class 'torch.Tensor'>  ; image.shape =  torch.Size([256, 256])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [162]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m Num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m train_losses, val_losses , model  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mNum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [161]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, model_name, optimizer, scheduler, num_epochs, data_loaders, patience)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     32\u001b[0m             loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 33\u001b[0m             \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     running_loss[phase] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss[phase] \u001b[38;5;241m/\u001b[39m dataset_sizes[phase]\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:162\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    158\u001b[0m             max_exp_avg_sqs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_exp_avg_sq\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    160\u001b[0m         state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 162\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m          \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m          \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m          \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m          \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m          \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m          \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m          \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m          \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m          \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m          \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:219\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 219\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\optim\\adamw.py:274\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    273\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[1;32m--> 274\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable:\n\u001b[0;32m    277\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model,optimizer,model_name=segformer(lr=0.0001)\n",
    "data_loaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "Num_epoch=200\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "\n",
    "train_losses, val_losses , model  = train_model(model,model_name, optimizer,scheduler,  Num_epoch,data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonne nouvelle, ça tourne. Maintenant, est-ce que ça retourne bien le résultat escompté ? Oui, j'ai affiché les graphiques obtenus dans la fonction __getitem__ et on a bien la transformation sur l'image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des transformées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A\n",
    "from tifffile import TiffFile, TiffWriter\n",
    "\n",
    "\n",
    "def show_image(image, display_min=0, display_max=2200, ax=None):\n",
    "\n",
    "    if image.dtype == np.uint16:\n",
    "        iscale = display_max - display_min\n",
    "        scale = 255 / iscale\n",
    "        byte_im = (image) * scale\n",
    "        byte_im = (byte_im.clip(0, 255) + 0.5).astype(np.uint8)\n",
    "        image = byte_im\n",
    "\n",
    "    return image\n",
    "\n",
    "def load_and_transform_image(image_path, transformation):\n",
    "\n",
    "    with TiffFile(image_path) as tifi:\n",
    "        image = tifi.asarray()[:,:,:3]\n",
    "\n",
    "    print(\"Image shape = \", image.shape)\n",
    "    transformed_image = transformation(image=image)['image']\n",
    "    return image, transformed_image\n",
    "\n",
    "\n",
    "# Chemin de l'image .tif\n",
    "image_path = \"D:/my_git/landscape_data/dataset/small_dataset/images/23.tif\"\n",
    "aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "        A.RandomRotate90(p=0.7),\n",
    "        A.Transpose(p=0.7),\n",
    "        A.OneOf([\n",
    "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=1),\n",
    "        A.GridDistortion(p=1),\n",
    "        A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "        ], p=0.8)])\n",
    "\n",
    "original_image, transformed_image = load_and_transform_image(image_path, aug)\n",
    "\n",
    "imor = show_image(original_image)\n",
    "imtr = show_image(transformed_image)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1), plt.imshow(imor)\n",
    "plt.subplot(1,2,2), plt.imshow(imtr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
