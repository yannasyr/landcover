{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.transform import resize\n",
    "from collections import OrderedDict\n",
    "from tifffile import TiffFile\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from transformers import SegformerModel, SegformerConfig\n",
    "from transformers import  SegformerForSemanticSegmentation\n",
    "\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import copy \n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On réécrit des fonctions utiles car les imports utilisent des parsers donc ça ne marche pas dans un Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverData():\n",
    "    \"\"\"Class to represent the S2GLC Land Cover Dataset for the challenge,\n",
    "    with useful metadata and statistics.\n",
    "    \"\"\"\n",
    "    # image size of the images and label masks\n",
    "    IMG_SIZE = 256\n",
    "    # the images are RGB+NIR (4 channels)\n",
    "    N_CHANNELS = 4\n",
    "    # we have 9 classes + a 'no_data' class for pixels with no labels (absent in the dataset)\n",
    "    N_CLASSES = 10\n",
    "    CLASSES = [\n",
    "        'no_data',\n",
    "        'clouds',\n",
    "        'artificial',\n",
    "        'cultivated',\n",
    "        'broadleaf',\n",
    "        'coniferous',\n",
    "        'herbaceous',\n",
    "        'natural',\n",
    "        'snow',\n",
    "        'water'\n",
    "    ]\n",
    "    # classes to ignore because they are not relevant. \"no_data\" refers to pixels without\n",
    "    # a proper class, but it is absent in the dataset; \"clouds\" class is not relevant, it\n",
    "    # is not a proper land cover type and images and masks do not exactly match in time.\n",
    "    IGNORED_CLASSES_IDX = [0, 1]\n",
    "\n",
    "    # The training dataset contains 18491 images and masks\n",
    "    # The test dataset contains 5043 images and masks\n",
    "    TRAINSET_SIZE = 18491\n",
    "    TESTSET_SIZE = 5043\n",
    "\n",
    "    # for visualization of the masks: classes indices and RGB colors\n",
    "    CLASSES_COLORPALETTE = {\n",
    "        0: [0,0,0],\n",
    "        1: [255,25,236],\n",
    "        2: [215,25,28],\n",
    "        3: [211,154,92],\n",
    "        4: [33,115,55],\n",
    "        5: [21,75,35],\n",
    "        6: [118,209,93],\n",
    "        7: [130,130,130],\n",
    "        8: [255,255,255],\n",
    "        9: [43,61,255]\n",
    "        }\n",
    "    CLASSES_COLORPALETTE = {c: np.asarray(color) for (c, color) in CLASSES_COLORPALETTE.items()}\n",
    "\n",
    "    # statistics\n",
    "    # the pixel class counts in the training set\n",
    "    TRAIN_CLASS_COUNTS = np.array(\n",
    "        [0, 20643, 60971025, 404760981, 277012377, 96473046, 333407133, 9775295, 1071, 29404605]\n",
    "    )\n",
    "    # the minimum and maximum value of image pixels in the training set\n",
    "    TRAIN_PIXELS_MIN = 1\n",
    "    TRAIN_PIXELS_MAX = 24356"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Y(mask2d):\n",
    "  occurrences = np.bincount(mask2d.flatten(), minlength=10)\n",
    "  Y = occurrences / np.sum(occurrences)\n",
    "  return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_parse_image_mask(image_path):\n",
    "    \"\"\"Load an image and its segmentation mask as numpy arrays and returning a tuple\n",
    "    Args:\n",
    "        image_path : path to image\n",
    "    Returns:\n",
    "        (numpy.array[uint16], numpy.array[uint8]): the image and mask arrays\n",
    "    \"\"\"\n",
    "    # image_path = Path(image_path)\n",
    "    # get mask path from image path:\n",
    "    # image should be in a images/<image_id>.tif subfolder, while the mask is at masks/<image_id>.tif\n",
    "    mask_path = image_path.replace(\"images\",\"masks\")\n",
    "    with TiffFile(image_path) as tifi, TiffFile(mask_path) as tifm:\n",
    "        image = tifi.asarray()[:, :, :4] \n",
    "        mask = tifm.asarray()\n",
    "        print(f\"Image shape: {image.shape}\")\n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On modifie la classe LandscapeData pour pouvoir insérer de la data augmentation sous conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandscapeData(Dataset):\n",
    "\n",
    "    def __init__(self, data_folder, transform=ToTensor()):\n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        # Liste des noms de fichiers dans les dossiers\n",
    "        image_files = os.listdir(os.path.join(data_folder, 'images'))\n",
    "\n",
    "        # Utilisez numpy_parse_image_mask pour charger les images et les masques\n",
    "        self.train_data = [numpy_parse_image_mask(os.path.join(data_folder, 'images', filename)) for filename in image_files]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, mask = self.train_data[idx]\n",
    "\n",
    "        image = image.astype(\"float32\")\n",
    "\n",
    "        seuil_per_water = 0.3\n",
    "        seuil_per_city = 0.3\n",
    "        seuil_per_natural = 0.3\n",
    "        seuil_per_conif = 0.6\n",
    "\n",
    "        Y = get_Y(mask) # Y[2] = 'artificial' ; Y[5] = 'coniferous' ; Y[7] = 'natural' ; Y[9] = 'water'\n",
    "\n",
    "        if Y[2] > seuil_per_city or Y[9] > seuil_per_water or Y[5] > seuil_per_conif  or Y[7] > seuil_per_natural :\n",
    "            augmented = self.transform_augm(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        else:\n",
    "            image = self.transform(image=image)['image']\n",
    "\n",
    "        mask = mask.astype(\"int64\")\n",
    "        mask = torch.tensor(mask, dtype=torch.int64) \n",
    "        mask = mask.squeeze()\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici nos transformées : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "means =  [ 418.19976217,  703.34810956,  663.22678147, 3253.46844222]\n",
    "stds =  [294.73191962, 351.31328415, 484.47475774, 793.73928079]\n",
    "\n",
    "# Transformations \n",
    "data_transforms = {\n",
    "    'train': A.Compose([\n",
    "        A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'train_augmentation': A.Compose([\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "        A.RandomRotate90(p=0.7),\n",
    "        A.Transpose(p=0.7),\n",
    "        A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.Normalize(means, stds),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "}\n",
    "\n",
    "# On pourrait rajouter dans l'augmentation cette transformation élastique, un peu violente ... ? \n",
    "# A.OneOf([\n",
    "# A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "# A.GridDistortion(p=0.5),\n",
    "# A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "# ], p=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On créé les datasets ```train```, ```validation```, ```test```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Image shape: (256, 256, 4)\n",
      "Number of images in the training set: 80\n",
      "Number of images in the validation set: 10\n",
      "Number of images in the test set: 10\n"
     ]
    }
   ],
   "source": [
    "# ------------- DATASET & DATALOADER ----------- \n",
    "\n",
    "# Définir le chemin du dossier d'entraînement\n",
    "data_folder = 'D:/my_git/landscape_data/dataset/small_dataset/'\n",
    "\n",
    "full_dataset = LandscapeData(data_folder, transform=None)  \n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = int(0.10 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_dataset.dataset.transform = data_transforms['train']\n",
    "train_dataset.dataset.transform_augm = data_transforms['train_augmentation']\n",
    "val_dataset.dataset.transform = data_transforms['test']\n",
    "test_dataset.dataset.transform = data_transforms['test']\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Number of images in train and val sets\n",
    "num_train_images = len(train_dataset)\n",
    "num_val_images = len(val_dataset)\n",
    "num_test_images = len(test_dataset)\n",
    "\n",
    "print(f\"Number of images in the training set: {num_train_images}\")\n",
    "print(f\"Number of images in the validation set: {num_val_images}\")\n",
    "print(f\"Number of images in the test set: {num_test_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance un entrainement avec le *segformer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segformer(lr=0.0001):\n",
    "    \n",
    "    config = SegformerConfig(\n",
    "        num_labels=10,\n",
    "        num_channels=4,\n",
    "        semantic_loss_ignore_index=0,\n",
    "        patch_sizes = [3, 2, 2, 2],\n",
    "        depths=[3, 4, 18, 3],\n",
    "        hidden_sizes=[64, 128, 320, 512],\n",
    "        decoder_hidden_size=768,\n",
    "    )\n",
    "    model_name =\"SegformerMit-B3\"\n",
    "\n",
    "    # Charger les poids pré-entrainés si le chemin est spécifié\n",
    "    model = SegformerForSemanticSegmentation(config)\n",
    "\n",
    "    # define optimizer\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr)\n",
    "\n",
    "    return model,optimizer,model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,model_name, optimizer,scheduler, num_epochs,data_loaders, patience=5):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    consecutive_epochs_no_improvement = 0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    dataset_sizes = {phase: len(data_loaders[phase].dataset) for phase in ['train', 'val']}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"Epoch {}/{}\".format(epoch, num_epochs - 1))\n",
    "        print(\"-\" * 10)\n",
    "\n",
    "        running_loss = {'train': 0.0, 'val': 0.0}\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            for inputs, targets in data_loaders[phase]:\n",
    "                pixel_values = inputs.to('cpu')\n",
    "                labels = targets.to('cpu')\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "                    loss, logits = outputs.loss, outputs.logits\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss[phase] += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss[phase] / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f}'.format(phase, epoch_loss))\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)  # Step the scheduler on validation loss\n",
    "\n",
    "\n",
    "            if phase == 'train':\n",
    "                train_losses.append(epoch_loss)\n",
    "            else:\n",
    "                val_losses.append(epoch_loss)\n",
    "\n",
    "                # Check for early stopping\n",
    "                if epoch_loss < best_val_loss:\n",
    "                    best_val_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    consecutive_epochs_no_improvement = 0\n",
    "                else:\n",
    "                    consecutive_epochs_no_improvement += 1\n",
    "\n",
    "        if consecutive_epochs_no_improvement >= patience:\n",
    "            print(f'Early stopping after {patience} consecutive epochs without improvement.')\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return train_losses, val_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/199\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m Num_epoch\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m\n\u001b[0;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m train_losses, val_losses , model  \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mNum_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdata_loaders\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, model_name, optimizer, scheduler, num_epochs, data_loaders, patience)\u001b[0m\n\u001b[0;32m     29\u001b[0m     loss, logits \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss, outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m phase \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 32\u001b[0m         \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m running_loss[phase] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\kille\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model,optimizer,model_name=segformer(lr=0.0001)\n",
    "data_loaders = {'train': train_loader, 'val': val_loader}\n",
    "\n",
    "Num_epoch=200\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.1)\n",
    "\n",
    "train_losses, val_losses , model  = train_model(model,model_name, optimizer,scheduler,  Num_epoch,data_loaders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affichage des transformées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import albumentations as A\n",
    "from tifffile import TiffFile, TiffWriter\n",
    "\n",
    "\n",
    "def show_image(image, display_min=0, display_max=2200, ax=None):\n",
    "\n",
    "    if image.dtype == np.uint16:\n",
    "        iscale = display_max - display_min\n",
    "        scale = 255 / iscale\n",
    "        byte_im = (image) * scale\n",
    "        byte_im = (byte_im.clip(0, 255) + 0.5).astype(np.uint8)\n",
    "        image = byte_im\n",
    "\n",
    "    return image\n",
    "\n",
    "def load_and_transform_image(image_path, transformation):\n",
    "\n",
    "    with TiffFile(image_path) as tifi:\n",
    "        image = tifi.asarray()[:,:,:3]\n",
    "\n",
    "    print(\"Image shape = \", image.shape)\n",
    "    transformed_image = transformation(image=image)['image']\n",
    "    return image, transformed_image\n",
    "\n",
    "\n",
    "# Chemin de l'image .tif\n",
    "image_path = \"D:/my_git/landscape_data/dataset/small_dataset/images/23.tif\"\n",
    "aug = A.Compose([\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "        A.RandomRotate90(p=0.7),\n",
    "        A.Transpose(p=0.7),\n",
    "        A.OneOf([\n",
    "        A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=1),\n",
    "        A.GridDistortion(p=1),\n",
    "        A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "        ], p=0.8)])\n",
    "\n",
    "original_image, transformed_image = load_and_transform_image(image_path, aug)\n",
    "\n",
    "imor = show_image(original_image)\n",
    "imtr = show_image(transformed_image)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1), plt.imshow(imor)\n",
    "plt.subplot(1,2,2), plt.imshow(imtr)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
